{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a OpenAI GYM\n",
    "**Ingeniería Electrónica**\n",
    "\n",
    "**Inteligencia Artificial**\n",
    "\n",
    "**19/06/2021**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar un algoritmo de Q-learning, usaremos el framework de OpenAI Gym, el cual es compatible con TensorFlow para desarrollar y comparar algoritmos de RL.\n",
    "\n",
    "OpenAI Gym consta de dos partes principales:\n",
    "* **La librería de código abierto Gym**: una colección de problemas y entornos que se pueden usar para probar los algoritmos de aprendizaje reforzado. Todos estos entornos tienen una interfaz compartida, lo que le permite escribir algoritmos RL.\n",
    "* **El servicio OpenAI Gym**: un sitio y API que permite a las personas comparar significativamente el rendimiento de sus agentes entrenados.\n",
    "\n",
    "Ver más referencias en https://gym.openai.com\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para instalar Gym, use el instalador pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gym\n",
    "#! conda install -c conda-forge gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez instalado, se pueden revisar los entornos de Gym de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "envs.registry.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada `EnvSpec` define una tarea para resolver, por ejemplo, la representación de `FrozenLake-v0` se da en la siguiente figura. El agente controla el movimiento de un personaje en un mundo de cuadrícula 4x4 (ver la siguiente figura). Algunas baldosas de la cuadrícula son transitables, y otras conducen al agente a caer al agua. Además, la dirección de movimiento del agente es incierta, y solo depende parcialmente de la dirección elegida. El agente es recompensado por encontrar una ruta transitable a una casilla de meta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cuadricula.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La superficie mostrada anteriormente se describe utilizando una cuadrícula, como la siguiente:\n",
    "\n",
    "SFFF (S: punto de partida, seguro)\n",
    "\n",
    "FHFH (F: superficie congelada, segura)\n",
    "\n",
    "FFFH (H: agujero, cae a tu perdición)\n",
    "\n",
    "HFFG (G: meta, donde se encuentra el objetivo)\n",
    "\n",
    "\n",
    "El *episodio* termina cuando alcanzamos la meta o caemos en un hoyo. Recibimos una recompensa de uno por alcanzar la meta y cero en caso contrario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema de implementación de FrozenLake-v0\n",
    "Aquí presentamos una implementación básica de Q-learning para el problema FrozenLake-v0.\n",
    "\n",
    "Importar las siguientes dos librerías básicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, cargamos el entorno `FrozenLake-v0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2021)\n",
    "environment = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, construimos la tabla Q-learning; tiene las dimensiones *SxA*, donde `S` es la dimensión del espacio de observación, mientras que `A` es la dimensión del espacio de acción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = environment.observation_space.n\n",
    "A = environment.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El entorno FrozenLake proporciona un estado para cada bloque y cuatro acciones (es decir, las cuatro direcciones de movimiento), lo que nos proporciona una tabla de valores Q de 16x4 para inicializar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros([S,A])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, definimos parámetros para la regla de entrenamiento y el factor de descuento. Establecemos el número total de episodios (pruebas). Luego, inicializamos `rList`, donde agregaremos la recompensa acumulada para evaluar la puntuación del algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .85\n",
    "alpha = .99\n",
    "gamma = .99\n",
    "num_episodes = 2000\n",
    "rList = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, comenzamos el ciclo de Q-learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_episodes):\n",
    "# Restablecer el entorno y obtener la primera observación nueva\n",
    "    s = environment.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "\n",
    "  # El algoritmo de aprendizaje Q-Table\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "\n",
    "   # Elegir una acción escogiendo (con ruido) de la tabla Q\n",
    "        a=np.argmax(Q[s,:]+ \\\n",
    "                  np.random.randn(1,environment.action_space.n)*(1./(i+1)))\n",
    "\n",
    "  # Obtener un nuevo estado y una recompensa del entorno\n",
    "        s1,r,d,_ = environment.step(a)\n",
    "\n",
    "        # Actualizar Q-Table con nuevos conocimientos\n",
    "        Q[s,a] = Q[s,a] + lr*(r + gamma *np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "\n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Puntuación en el tiempo: \" + str(sum(rList)/num_episodes))\n",
    "print(\"Valores finales de la tabla Q\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La recompensa promedio es de aproximadamente 0.47 en 100 pruebas consecutivas. Técnicamente, no lo resolvimos. De hecho, `FrozenLake-v0` define la resolución como obtener una recompensa promedio de 0.78 en más de 100 pruebas consecutivas; podrían mejorar este resultado ajustando los parámetros de configuración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning con TensorFlow\n",
    "En el ejemplo anterior, vimos cómo es relativamente simple, utilizando una cuadrícula de 16x4, actualizar la tabla Q en cada paso del proceso de aprendizaje. Es fácil imaginar que el uso de esta tabla puede servir para problemas simples, pero en problemas del mundo real, necesitamos un mecanismo más sofisticado para actualizar el estado del sistema. Este es el punto donde interviene el Deep Learning. Las redes neuronales son excepcionalmente buenas para generar buenas características para datos altamente estructurados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veremos cómo usar una función Q con una red neuronal, que toma el estado y la acción como entrada y emite el valor Q correspondiente. Para hacer eso, construiremos una red de una capa que tome el estado, codificado en un vector [1x16], que aprenda el mejor movimiento (acción), mapeando las acciones posibles en un vector de longitud cuatro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una aplicación de redes Q profundas (**_deep Q-networks_**) ha tenido éxito en jugar algunos juegos de Atari 2600 a niveles humanos expertos. Los resultados preliminares se presentaron en 2014, con un artículo publicado en febrero de 2015, en *Nature*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se describe una implementación basada en TensorFlow de una red neuronal Q-learning para el problema `FrozenLake-v0`.\n",
    "\n",
    "Importar las librerías necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje de episodios exitosos: 0.584%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Definir el entorno FrozenLake\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "# Configurar las variables de TensorFlow\n",
    "tf.reset_default_graph()\n",
    "inputs1 = tf.placeholder(shape=[1,16],dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([16,4],0,0.01))\n",
    "Qout = tf.matmul(inputs1,W)\n",
    "predict = tf.argmax(Qout,1)\n",
    "nextQ = tf.placeholder(shape=[1,4],dtype=tf.float32)\n",
    "\n",
    "# definir las funciones de pérdida y optimización\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)\n",
    "\n",
    "#initilizar las variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# preparar los parámetros de q-learning\n",
    "gamma = .99\n",
    "e = 0.1\n",
    "num_episodes = 6000\n",
    "jList = []\n",
    "rList = []\n",
    "\n",
    "# Ejecutar la sesión\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "# Iniciar el procedimiento de Q-learning\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        while j < 99:\n",
    "            j+=1\n",
    "            a,allQ = sess.run([predict,Qout],\\\n",
    "                              feed_dict=\\\n",
    "                              {inputs1:np.identity(16)[s:s+1]})\n",
    "\n",
    "            if np.random.rand(1) < e:\n",
    "                a[0] = env.action_space.sample()\n",
    "            s1,r,d,_ = env.step(a[0])\n",
    "            Q1 = sess.run(Qout,feed_dict=\\\n",
    "                          {inputs1:np.identity(16)[s1:s1+1]})\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0,a[0]] = r + gamma *maxQ1\n",
    "            _,W1 = sess.run([updateModel,W],\\\n",
    "                            feed_dict=\\\n",
    "                           {inputs1:np.identity(16)[s:s+1],nextQ:targetQ})\n",
    "# acumular la recompensa total\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            if d == True:\n",
    "                e = 1./((i/50) + 10)\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "# imprimir los resultados\n",
    "    print(\"Porcentaje de episodios exitosos: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todavía se podría mejorar ajustando los parámetros de la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otros ejemplos con OpenAI\n",
    "### implementación de CartPole-v1\n",
    "\n",
    "Un tubo está unido por una articulación no accionada a un carro, que se mueve a lo largo de una pista sin fricción. El sistema se controla aplicando una fuerza de +1 o -1 al carro. El péndulo comienza en posición vertical, y el objetivo es evitar que se caiga. Se proporciona una recompensa de +1 por cada paso de tiempo que el tubo permanece en posición vertical. El episodio termina cuando el tubo está a más de 15 grados de la vertical, o el si el carro se mueve a más de 2.4 unidades del centro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation = env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "action = env.action_space.sample()  # el agente toma acciones aleatorias\n",
    "observation, reward, done, info = env.step(action)\n",
    "if done:\n",
    "    observation = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La segunda línea crea un entorno `CartPole-v1`.\n",
    "\n",
    "La tercera línea inicializa los parámetros de estado.\n",
    "\n",
    "La quinta línea muestra el juego.\n",
    "\n",
    "La sexta línea genera una acción con una política que es \"aleatoria\".\n",
    "\n",
    "En la séptima línea, se toma la acción y el entorno ofrece cuatro resultados:\n",
    "* **Observation**: Parámetros del estado del juego. Diferentes juegos devuelven diferentes parámetros. En CartPole, hay cuatro en total. El segundo parámetro es el ángulo del poste.\n",
    "* **Reward**: la puntuación que obtienes después de realizar esta acción.\n",
    "* **Done**: Juego terminado o no terminado.\n",
    "* **Info**: información adicional de depuración. Podría estar haciendo trampa.\n",
    "\n",
    "La octava línea significa si el juego está terminado; reiniciarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La política se puede programar de la forma que se desee. Se puede basar en reglas if-else o en una red neuronal. Aquí hay una pequeña demostración simple, con la política más simple para el juego CartPole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "def policy(observation):\n",
    "    angle = observation[2]\n",
    "    if angle < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation = env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    action = policy(observation)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
