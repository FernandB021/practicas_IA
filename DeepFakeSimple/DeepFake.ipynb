{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepFake Simple\n",
    "**Ingeniería Electrónica**\n",
    "\n",
    "**Inteligencia Artificial**\n",
    "\n",
    "**19/06/2021**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En una publicación reciente (2019), parte de *Advances in Neural Information Processing Systems 32 (NIPS 2019)*, llamada **_First Order Motion Model for Image Animation_**. En este artículo, los autores, Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci y Nicu Sebe, presentan una forma novedosa de animar una imagen fuente dada un video, sin ninguna información adicional o anotación sobre el objeto a animar.\n",
    "\n",
    "Empelan una red neuronal entrenada para reconstruir un video, dado un cuadro fuente (imagen fija) y una representación latente del movimiento en el video, que se aprende durante el entrenamiento. En el momento de la prueba, el modelo toma como entrada una nueva imagen fuente y un video (por ejemplo, una secuencia de cuadros) y predice cómo se mueve el objeto en la imagen fuente de acuerdo con el movimiento representado en estos cuadros.\n",
    "\n",
    "El modelo rastrea todo lo que es interesante en una animación: movimientos de la cabeza, conversación, seguimiento ocular e incluso acción corporal. El código fuente está disonible en su repositorio https://github.com/AliaksandrSiarohin/first-order-model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"framework.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, utilizaremos un *wrapper*, un script simple desarrrolado por Dimitris Poulopoulos, basado en la publicación antes mencionada, con el cual cualquier desarrolador puede usar para producir DeepFakes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ejecutarlo, se debe instalar las siguientes librerías y realizar los siguientes pasos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. conda update numpy\n",
    "2. pip install -f https://download.pytorch.org/whl/torch_stable.html torch===1.4.0\n",
    "3. conda install ffmpeg -c conda-forge\n",
    "4. pip install fastscript\n",
    "5. pip install deep-animator --user\n",
    "6. Agregar a variable de entorno PATH --> C:\\Users\\USUARIO\\AppData\\Roaming\\Python\\Python37\\Scripts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, necesitamos cuatro elementos:\n",
    "* Los pesos del modelo; por supuesto, no queremos entrenar al modelo desde cero. Por lo tanto, necesitamos los pesos para cargar un modelo pre-entrenado.\n",
    "* Un archivo de configuración YAML para nuestro modelo.\n",
    "* Una imagen fuente; Esto podría ser, por ejemplo, un retrato.\n",
    "* Un video; Lo mejor es descargar un video con una cara claramente visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener algunos resultados rápidamente y probar el rendimiento del algoritmo, puede usamos imagen fuente y video de ejemplos. Los pesos del modelo se encuentran en `vox-cpk.pth.tar` y se descargan de: https://drive.google.com/drive/folders/1PyQJmkdCsAkOYwUyaj_l-l0as-iLDgeH\n",
    "\n",
    "A continuación se muestra un archivo de configuración YAML simple. En un editor de texto, se puede copiar y pegar lo siguiente y se guarda como `conf.yml`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model_params:\n",
    "  common_params:\n",
    "    num_kp: 10\n",
    "    num_channels: 3\n",
    "    estimate_jacobian: True\n",
    "  kp_detector_params:\n",
    "     temperature: 0.1\n",
    "     block_expansion: 32\n",
    "     max_features: 1024\n",
    "     scale_factor: 0.25\n",
    "     num_blocks: 5\n",
    "  generator_params:\n",
    "    block_expansion: 64\n",
    "    max_features: 512\n",
    "    num_down_blocks: 2\n",
    "    num_bottleneck_blocks: 6\n",
    "    estimate_occlusion_map: True\n",
    "    dense_motion_params:\n",
    "      block_expansion: 64\n",
    "      max_features: 1024\n",
    "      num_blocks: 5\n",
    "      scale_factor: 0.25\n",
    "  discriminator_params:\n",
    "    scales: [1]\n",
    "    block_expansion: 32\n",
    "    max_features: 512\n",
    "    num_blocks: 4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecuar el algortimo con los archivos de ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! deep_animate 00.png 00.mp4 conf.yml vox-cpk.pth.tar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
